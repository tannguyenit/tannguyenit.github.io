---
layout: post
title:  "Prompt l√† g√¨? LLM ho·∫°t ƒë·ªông ra sao? V√† l√†m sao ƒë·ªÉ LLM c√≥ d·ªØ li·ªáu m·ªõi nh·∫•t?"
author: tannguyen
categories: [ LLM ]
# image: assets/images/post/2025/fastAPI-vs-Laravel.png
description: "LLM"
featured: false
hidden: false
---

> B√†i vi·∫øt n√†y t·ªïng h·ª£p l·∫°i to√†n b·ªô flow t·ª´ Prompt ‚Üí LLM ‚Üí Token ‚Üí Transformer ‚Üí Sampling ‚Üí Output, v√† gi·∫£i th√≠ch c√°ch ƒë·ªÉ LLM c√≥ th·ªÉ tr·∫£ l·ªùi d·ªØ li·ªáu m·ªõi nh·∫•t (RAG, Fine-tune, Retrain). N·ªôi dung d√†nh cho developer mu·ªën hi·ªÉu b·∫£n ch·∫•t thay v√¨ ch·ªâ d√πng API.

* * *

1\. Prompt l√† g√¨?
-----------------

**Prompt** l√† input (ƒë·∫ßu v√†o) m√† ch√∫ng ta ƒë∆∞a v√†o m·ªôt Large Language Model (LLM) ƒë·ªÉ n√≥ sinh ra output. Prompt v·ªÅ b·∫£n ch·∫•t ch·ªâ l√† m·ªôt **string**.

V√≠ d·ª•:

    "Ronaldo l√† ai?"

Prompt c√≥ th·ªÉ ch·ª©a:

*   c√¢u h·ªèi
*   ng·ªØ c·∫£nh
*   format mong mu·ªën
*   rule / constraint
*   persona / role

N√≥i ƒë∆°n gi·∫£n:
**Prompt l√† "c√°i m√† m√¨nh nh√©t v√†o model" ƒë·ªÉ model tr·∫£ l·ªùi ƒë√∫ng √Ω m√¨nh.**

2\. Prompt Engineering l√† g√¨?
-----------------------------

**Prompt Engineering** l√† k·ªπ thu·∫≠t thi·∫øt k·∫ø prompt sao cho:

*   model hi·ªÉu ƒë√∫ng √Ω
*   output ƒë√∫ng expectation
*   tr√°nh tr·∫£ l·ªùi sai ho·∫∑c lan man
*   format output ƒë√∫ng chu·∫©n
*   c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng (reusable)

### Prompt Engineering gi·ªëng code nh∆∞ th·∫ø n√†o?

C√≥ th·ªÉ hi·ªÉu gi·ªëng nh∆∞ l·∫≠p tr√¨nh:

*   **Input** = prompt
*   **Business logic** = model reasoning (·∫©n trong LLM)
*   **Output** = response

**Prompt engineering = vi·∫øt prompt sao cho "ch·∫°y ƒë∆∞·ª£c" gi·ªëng nh∆∞ code ch·∫°y ƒë√∫ng.**

* * *

3\. LLM l√† g√¨? B·∫£n ch·∫•t c·ªßa LLM ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?
------------------------------------------------------

LLM (Large Language Model) l√† m·ªôt AI model ƒë∆∞·ª£c train tr√™n l∆∞·ª£ng text c·ª±c l·ªõn (internet, s√°ch, t√†i li·ªáu‚Ä¶).

Nh∆∞ng b·∫£n ch·∫•t c·ªßa LLM r·∫•t ƒë∆°n gi·∫£n:

> LLM l√† h·ªá th·ªëng d·ª± ƒëo√°n token ti·∫øp theo d·ª±a tr√™n x√°c su·∫•t.

4\. Token l√† g√¨?
-----------------

LLM **kh√¥ng ƒë·ªçc ch·ªØ theo ki·ªÉu con ng∆∞·ªùi ƒë·ªçc word**, m√† n√≥ ƒë·ªçc theo **token**.

üëâ **Token** l√† m·ªôt ƒë∆°n v·ªã nh·ªè m√† model d√πng ƒë·ªÉ x·ª≠ l√Ω input/output.

Token c√≥ th·ªÉ l√†:

*   m·ªôt t·ª´ (`football`)
*   m·ªôt ph·∫ßn c·ªßa t·ª´ (`foot` + `ball`)
*   d·∫•u c√°ch ( )
*   d·∫•u ch·∫•m (`.`)
*   k√Ω t·ª± ƒë·∫∑c bi·ªát (`?`, `!`)
*   ti·∫øng Vi·ªát c√≥ d·∫•u s·∫Ω b·ªã t√°ch kh√°c nhau t√πy tokenizer

### V√≠ d·ª•

B·∫°n nh·∫≠p:

    Ronaldo l√† ai?

Model c√≥ th·ªÉ tokenize th√†nh ki·ªÉu nh∆∞:

    ["Ronaldo", " l√†", " ai", "?"]

Ho·∫∑c c√≥ th·ªÉ chi ti·∫øt h∆°n:

    ["Ron", "aldo", " l√†", " ai", "?"]

_(L∆∞u √Ω: ƒë√¢y ch·ªâ l√† minh h·ªça, token th·ª±c t·∫ø ph·ª• thu·ªôc v√†o tokenizer c·ªßa t·ª´ng model.)_

### Token quan tr·ªçng v√¨ sao?

LLM ho·∫°t ƒë·ªông theo c∆° ch·∫ø: **d·ª± ƒëo√°n token ti·∫øp theo, kh√¥ng ph·∫£i d·ª± ƒëo√°n t·ª´ ti·∫øp theo.**

N√™n b·∫£n ch·∫•t LLM gi·ªëng nh∆∞:

    Input tokens -> model -> output token ti·∫øp theo -> n·ªëi v√†o -> repeat

N√≥ gi·ªëng autocomplete, nh∆∞ng m·∫°nh h∆°n r·∫•t nhi·ªÅu.

### Token c√≥ ph·∫£i k√Ω t·ª± kh√¥ng?

Kh√¥ng.

*   1 token c√≥ th·ªÉ l√† 1 ch·ªØ
*   ho·∫∑c m·ªôt c·ª•m ch·ªØ
*   ho·∫∑c n·ª≠a ch·ªØ

V√≠ d·ª• ti·∫øng Anh:

    unbelievable

C√≥ th·ªÉ th√†nh:

    ["un", "believ", "able"]

5\. Context Window l√† g√¨?
--------------------------

**Context Window** l√† gi·ªõi h·∫°n b·ªô nh·ªõ ng·∫Øn h·∫°n c·ªßa model trong m·ªôt l·∫ßn chat.

üëâ Model c√≥ th·ªÉ "nh√¨n th·∫•y" t·ªëi ƒëa bao nhi√™u token trong input.

M·ªôt s·ªë m·ª©c context window ph·ªï bi·∫øn:

*   4k tokens
*   8k tokens
*   32k tokens
*   128k tokens

N·∫øu v∆∞·ª£t qu√° gi·ªõi h·∫°n, model s·∫Ω kh√¥ng th·ªÉ ƒë·ªçc h·∫øt to√†n b·ªô cu·ªôc h·ªôi tho·∫°i ho·∫∑c t√†i li·ªáu.

### Context Window bao g·ªìm nh·ªØng g√¨?

Context window kh√¥ng ch·ªâ g·ªìm c√¢u h·ªèi c·ªßa b·∫°n, m√† g·ªìm to√†n b·ªô:

*   System prompt (instruction g·ªëc)
*   Chat history (c√°c tin nh·∫Øn tr∆∞·ªõc ƒë√≥)
*   Prompt hi·ªán t·∫°i
*   D·ªØ li·ªáu b·∫°n nh√©t v√†o (RAG documents, tool outputs)
*   V√† c·∫£ output m√† model ƒëang generate

üëâ T·∫•t c·∫£ ƒë·ªÅu t√≠nh v√†o token budget.

6\. T·∫°i sao Context Window l·∫°i quan tr·ªçng?
-------------------------------------------

V√¨ n·∫øu b·∫°n chat d√†i qu√°:

*   model s·∫Ω b·∫Øt ƒë·∫ßu **qu√™n c√°c tin nh·∫Øn c≈©**
*   ho·∫∑c framework s·∫Ω t·ª± ƒë·ªông **c·∫Øt b·ªõt history**
*   d·∫´n t·ªõi vi·ªác model tr·∫£ l·ªùi ki·ªÉu m·∫•t logic, qu√™n context

V√≠ d·ª• b·∫°n chat 30 ph√∫t, t·ªõi ph√∫t 31 b·∫°n h·ªèi:

    "c√≤n c√°i √Ω s·ªë 3 h·ªìi n√£y?"

Model c√≥ th·ªÉ tr·∫£ l·ªùi sai v√¨ **√Ω s·ªë 3 ƒë√£ b·ªã r·ªõt kh·ªèi context window**.

7\. LLM c√≥ "memory" kh√¥ng?
---------------------------

B·∫£n ch·∫•t LLM **kh√¥ng c√≥ memory l√¢u d√†i**.

N√≥ ch·ªâ c√≥:

*   **Context window** (short-term memory)
*   **Weights** (ki·∫øn th·ª©c ƒë√£ train t·ª´ tr∆∞·ªõc)

N·∫øu d·ªØ li·ªáu n·∫±m ngo√†i context window, model g·∫ßn nh∆∞ coi nh∆∞ kh√¥ng bi·∫øt.

8\. M·ªëi li√™n h·ªá gi·ªØa Token v√† Context Window
---------------------------------------------

Context window ƒë∆∞·ª£c ƒëo b·∫±ng token.

V√≠ d·ª• model c√≥ context window = **8192 tokens**, nghƒ©a l√†:

üëâ t·ªïng input + history + output kh√¥ng ƒë∆∞·ª£c v∆∞·ª£t qu√° ~8192 tokens.

9\. Context Window = "RAM" c·ªßa LLM
-----------------------------------

Context window gi·ªëng RAM h∆°n l√† ·ªï c·ª©ng:

*   **RAM:** nhanh nh∆∞ng gi·ªõi h·∫°n
*   **·ªî c·ª©ng:** l∆∞u l√¢u nh∆∞ng model kh√¥ng t·ª± truy c·∫≠p ƒë∆∞·ª£c

üëâ Context window l√† RAM.

C√≤n d·ªØ li·ªáu "th·∫ø gi·ªõi" c·ªßa model n·∫±m trong weights (gi·ªëng firmware/ROM).

10\. V√≠ d·ª• th·ª±c t·∫ø: Context Window b·ªã c·∫Øt
------------------------------------------

Gi·∫£ s·ª≠ model context window ch·ªâ c√≥ **10 tokens** (v√≠ d·ª• gi·∫£ l·∫≠p).

B·∫°n chat nh∆∞ sau:

    B·∫°n l√† tr·ª£ l√Ω AI.
    Ronaldo l√† ai?

N·∫øu tokenizer bi·∫øn th√†nh 12 tokens ‚Üí v∆∞·ª£t 10 tokens ‚Üí h·ªá th·ªëng ph·∫£i **c·∫Øt b·ªõt**.

V√≠ d·ª• n√≥ c·∫Øt m·∫•t c√¢u:

    B·∫°n l√† tr·ª£ l√Ω AI.

Th√¨ model kh√¥ng c√≤n bi·∫øt vai tr√≤ n·ªØa. ƒê√≥ l√† l√Ω do ƒë√¥i khi b·∫°n th·∫•y model:

*   l√∫c ƒë·∫ßu n√≥i l·ªãch s·ª±
*   sau ƒë√≥ t·ª± nhi√™n m·∫•t style

V√¨ instruction b·ªã drop kh·ªèi context window.

11\. Hallucination
---------------------------------------------------------

Hallucination l√† hi·ªán t∆∞·ª£ng LLM tr·∫£ l·ªùi **r·∫•t t·ª± tin** nh∆∞ng n·ªôi dung l·∫°i **sai** ho·∫∑c **kh√¥ng c√≥ th·∫≠t**. M·ªôt trong nh·ªØng nguy√™n nh√¢n ph·ªï bi·∫øn nh·∫•t d·∫´n t·ªõi hallucination ch√≠nh l√† vi·ªác **context window b·ªã ƒë·∫ßy** v√† h·ªá th·ªëng ph·∫£i **c·∫Øt b·ªõt token**.

### 1) Khi token b·ªã c·∫Øt th√¨ ƒëi·ªÅu g√¨ x·∫£y ra?

Trong m·ªôt cu·ªôc h·ªôi tho·∫°i d√†i, prompt kh√¥ng ch·ªâ bao g·ªìm c√¢u h·ªèi hi·ªán t·∫°i m√† c√≤n bao g·ªìm:

*   System prompt (instruction ban ƒë·∫ßu)
*   L·ªãch s·ª≠ chat (c√°c message tr∆∞·ªõc ƒë√≥)
*   D·ªØ li·ªáu t·ª´ RAG (t√†i li·ªáu ƒë∆∞·ª£c nh√©t v√†o prompt)
*   C√°c output c≈©

Khi t·ªïng s·ªë token v∆∞·ª£t qu√° gi·ªõi h·∫°n context window, h·ªá th·ªëng s·∫Ω ph·∫£i th·ª±c hi·ªán **truncate** (c·∫Øt b·ªõt m·ªôt ph·∫ßn n·ªôi dung c≈©).

Th√¥ng th∆∞·ªùng, c√°c ƒëo·∫°n b·ªã c·∫Øt s·∫Ω l√† ph·∫ßn c≈© nh·∫•t trong conversation.

### 2) V√¨ sao token b·ªã c·∫Øt d·ªÖ g√¢y hallucination?

LLM kh√¥ng ph·∫£i l√† m·ªôt h·ªá th·ªëng "search database" ho·∫∑c "lookup knowledge". N√≥ ho·∫°t ƒë·ªông b·∫±ng c√°ch:

    Nh√¨n context hi·ªán t·∫°i ‚Üí d·ª± ƒëo√°n token ti·∫øp theo ‚Üí n·ªëi v√†o output ‚Üí l·∫∑p l·∫°i

N·∫øu c√°c token quan tr·ªçng b·ªã c·∫Øt (v√≠ d·ª•: ƒë·ªãnh nghƒ©a, rule business, d·ªØ li·ªáu g·ªëc), th√¨ model s·∫Ω **kh√¥ng c√≤n ƒë·ªß th√¥ng tin** ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c.

Tuy nhi√™n, LLM v·∫´n ph·∫£i ti·∫øp t·ª•c generate output. V√† v√¨ b·∫£n ch·∫•t c·ªßa n√≥ l√† m·ªôt h·ªá th·ªëng d·ª± ƒëo√°n x√°c su·∫•t, n√≥ s·∫Ω ch·ªçn c√¢u tr·∫£ l·ªùi "c√≥ v·∫ª h·ª£p l√Ω nh·∫•t" d·ª±a v√†o c√°c pattern trong d·ªØ li·ªáu training.

üëâ K·∫øt qu·∫£ l√† model c√≥ th·ªÉ **t·ª± b·ªãa ra c√¢u tr·∫£ l·ªùi**. ƒê√≥ ch√≠nh l√† hallucination.

### 3) V√≠ d·ª• hallucination do context window b·ªã c·∫Øt

Gi·∫£ s·ª≠ ·ªü ƒë·∫ßu cu·ªôc chat b·∫°n ƒë·ªãnh nghƒ©a:

    Trong h·ªá th·ªëng n√†y, user role = ADMIN th√¨ ƒë∆∞·ª£c x√≥a tenant.

Sau khi chat d√†i, ƒëo·∫°n n√†y b·ªã c·∫Øt kh·ªèi context window.

B·∫°n h·ªèi ti·∫øp:

    Role STAFF c√≥ ƒë∆∞·ª£c x√≥a tenant kh√¥ng?

V√¨ model kh√¥ng c√≤n th·∫•y rule ban ƒë·∫ßu n·ªØa, n√≥ s·∫Ω tr·∫£ l·ªùi theo ki·ªÉu suy lu·∫≠n ph·ªï bi·∫øn:

    Kh√¥ng, STAFF th∆∞·ªùng kh√¥ng c√≥ quy·ªÅn x√≥a tenant.

Nh∆∞ng n·∫øu business rule th·∫≠t s·ª± c·ªßa b·∫°n l√†:

    STAFF v·∫´n ƒë∆∞·ª£c x√≥a tenant n·∫øu c√≥ permission flag ƒë·∫∑c bi·ªát.

Th√¨ model ƒë√£ tr·∫£ l·ªùi sai, nh∆∞ng l·∫°i tr·∫£ l·ªùi r·∫•t t·ª± tin.

üëâ ƒê√¢y ch√≠nh l√† hallucination.

### 4) Hallucination kh√¥ng ph·∫£i bug, m√† l√† b·∫£n ch·∫•t

Hallucination kh√¥ng h·∫≥n l√† "l·ªói ph·∫ßn m·ªÅm", m√† l√† h·∫≠u qu·∫£ t·ª± nhi√™n c·ªßa c√°ch LLM ho·∫°t ƒë·ªông:

*   LLM kh√¥ng truy c·∫≠p database th·∫≠t
*   LLM kh√¥ng verify fact
*   LLM ch·ªâ d·ª± ƒëo√°n token ti·∫øp theo d·ª±a tr√™n context ƒëang c√≥

N·∫øu context b·ªã thi·∫øu, model s·∫Ω t·ª± "ƒëi·ªÅn" ph·∫ßn c√≤n thi·∫øu b·∫±ng ki·∫øn th·ª©c th·ªëng k√™ ƒë√£ h·ªçc.

### 5) M·ªôt s·ªë d·∫°ng hallucination ph·ªï bi·∫øn do token b·ªã c·∫Øt

*   **Qu√™n format output:** b·∫°n y√™u c·∫ßu output JSON nh∆∞ng ƒëo·∫°n instruction b·ªã c·∫Øt ‚Üí model tr·∫£ v·ªÅ text b√¨nh th∆∞·ªùng.
*   **B·ªãa ra s·ªë li·ªáu:** b·∫°n h·ªèi doanh thu th√°ng 1 nh∆∞ng d·ªØ li·ªáu ƒë√£ b·ªã c·∫Øt kh·ªèi context ‚Üí model t·ª± ƒëo√°n.
*   **B·ªãa t√™n class/function:** b·∫°n n√≥i c√≥ class `NotificationProcessor` nh∆∞ng b·ªã c·∫Øt ‚Üí model t·ª± sinh ra class kh√°c.
*   **B·ªãa logic business:** model m·∫•t rule ban ƒë·∫ßu ‚Üí suy lu·∫≠n theo logic ph·ªï bi·∫øn nh∆∞ng sai v·ªõi h·ªá th·ªëng c·ªßa b·∫°n.

### 6) T√≥m t·∫Øt

    Token b·ªã c·∫Øt ‚Üí m·∫•t d·ªØ li·ªáu input ‚Üí model thi·∫øu context ‚Üí model ƒëo√°n theo th·ªëng k√™ ‚Üí hallucination

N√™n hallucination ƒë√¥i khi ch·ªâ ƒë∆°n gi·∫£n l√†:

> "LLM ƒëang tr·∫£ l·ªùi m·ªôt c√¢u h·ªèi m√† n√≥ kh√¥ng c√≤n ƒë·ªß context ƒë·ªÉ tr·∫£ l·ªùi ƒë√∫ng."

* * *

12\. V√≠ d·ª• th·ª±c t·∫ø: Khi h·ªèi "Ronaldo l√† ai?" th√¨ chuy·ªán g√¨ x·∫£y ra?
------------------------------------------------------------

User nh·∫≠p:

    Ronaldo l√† ai?

LLM s·∫Ω ch·∫°y pipeline n·ªôi b·ªô:

*   tokenize input
*   bi·∫øn token th√†nh vector
*   transformer x·ª≠ l√Ω context
*   d·ª± ƒëo√°n token ti·∫øp theo
*   l·∫∑p ƒë·∫øn khi ƒë·ªß c√¢u tr·∫£ l·ªùi

13\. Step-by-step Flow khi LLM nh·∫≠n c√¢u h·ªèi
--------------------------------------------

*   **Step 1:** Nh·∫≠n prompt
*   **Step 2:** Tokenization
*   **Step 3:** Embedding
*   **Step 4:** Transformer x·ª≠ l√Ω ng·ªØ c·∫£nh
*   **Step 5:** Predict token ti·∫øp theo
*   **Step 6:** Sampling
*   **Step 7:** Append token v√† l·∫∑p
*   **Step 8:** Stop condition

14\. Diagram Flow LLM
---------------------

    USER INPUT
       ‚Üì
    TOKENIZATION
       ‚Üì
    EMBEDDING
       ‚Üì
    TRANSFORMER
       ‚Üì
    LOGITS
       ‚Üì
    SOFTMAX
       ‚Üì
    SAMPLING
       ‚Üì
    NEXT TOKEN
       ‚Üì
    LOOP
       ‚Üì
    FINAL RESPONSE

15\. X√°c su·∫•t token d·ª±a v√†o ƒë√¢u?
---------------------------------

Model ƒë∆∞·ª£c train b·∫±ng c√°ch ƒë·ªçc h√†ng t·ª∑ c√¢u text v√† h·ªçc d·ª± ƒëo√°n token ti·∫øp theo.

X√°c su·∫•t token n·∫±m trong kho·∫£ng **0 ƒë·∫øn 1** v√† t·ªïng x√°c su·∫•t c·ªßa to√†n b·ªô token lu√¥n b·∫±ng **1**.

* * *

16\. L√†m sao ƒë·ªÉ LLM c√≥ d·ªØ li·ªáu m·ªõi nh·∫•t?
------------------------------------------

LLM kh√¥ng t·ª± update realtime. C√≥ 3 c√°ch:

*   **Train l·∫°i model** (r·∫•t t·ªën k√©m)
*   **Fine-tune** (h·ªçc th√™m domain/style)
*   **RAG** (ph·ªï bi·∫øn nh·∫•t)

17\. Diagram ki·∫øn tr√∫c RAG
---------------------------

    USER
      ‚Üì
    BACKEND API
      ‚Üì
    DATABASE / VECTOR DB
      ‚Üì
    RETRIEVED CONTEXT
      ‚Üì
    PROMPT CONSTRUCTION
      ‚Üì
    LLM
      ‚Üì
    FINAL RESPONSE

* * *

18\. Prompt Template chu·∫©n cho Developer
-----------------------------------------

### Template 1: Role + Task + Constraints

    You are a senior software engineer.

    Task:
    Explain what a database index is.

    Constraints:
    - Use Vietnamese
    - Use bullet points
    - Provide at least 1 real SQL example
    - Keep it short (max 200 words)

    Output format:
    - Definition
    - Example
    - Notes

### Template 2: System + Context + Question

    System:
    You are an expert assistant.

    Context:
    We are building a NestJS backend. The database is PostgreSQL.

    Question:
    How to optimize queries with indexes?

    Rules:
    - Mention B-Tree
    - Mention EXPLAIN ANALYZE
    - Give example query

### Template 3: Few-shot

    You are an assistant that converts user requirements into SQL.

    Example:
    User: List all employees
    Output:
    SELECT * FROM employees;

    Example:
    User: List employees older than 30
    Output:
    SELECT * FROM employees WHERE age > 30;

    Now answer this:
    User: List employees in department IT older than 25
    Output:

### Template 4: Prompt chu·∫©n cho RAG

    You are a helpful assistant.

    Use ONLY the following context to answer the question.
    If the answer is not in the context, say "I don't know".

    Context:
    {{context}}

    Question:
    {{question}}

    Answer:

* * *

19\. Ollama l√† g√¨?
-------------------

Ollama l√† tool gi√∫p ch·∫°y LLM local nhanh v√† expose API t·∫°i localhost.

20\. HuggingFace Transformers l√† g√¨?
--------------------------------------

HuggingFace Transformers l√† th∆∞ vi·ªán Python gi√∫p load model, tokenize, ch·∫°y inference, debug tokenization/logits.

21\. Ollama vs HuggingFace
---------------------------

| Ti√™u ch√≠                | Ollama   | HuggingFace Transformers |
| ----------------------- | -------- | ------------------------ |
| D·ªÖ c√†i                  | ‚úÖ R·∫•t d·ªÖ | ‚ùå ph·ª©c t·∫°p h∆°n           |
| Ch·∫°y model local nhanh  | ‚úÖ        | ‚úÖ                        |
| D·ªÖ g·ªçi API              | ‚úÖ        | ‚ùå                        |
| Debug token, logits     | ‚ùå kh√≥    | ‚úÖ r·∫•t m·∫°nh               |
| Ph√π h·ª£p demo / test     | ‚úÖ        | ‚ö†Ô∏è                       |
| Ph√π h·ª£p nghi√™n c·ª©u flow | ‚ùå        | ‚úÖ                        |

22\. Code m·∫´u g·ªçi Ollama API
------------------------------

    import requests

    url = "http://localhost:11434/api/generate"

    payload = {
        "model": "llama3.2",
        "prompt": "Ronaldo l√† ai?",
        "stream": False
    }

    response = requests.post(url, json=payload)
    data = response.json()

    print("Response:", data["response"])

23\. Code debug HuggingFace (token + logits)
---------------------------------------------

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM

    MODEL_NAME = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

    prompt = "Who is Obama?"
    inputs = tokenizer(prompt, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    last_token_logits = logits[0, -1]

    probs = torch.softmax(last_token_logits, dim=-1)
    top_probs, top_indices = torch.topk(probs, 5)

    print("Top 5 next token predictions:")
    for prob, idx in zip(top_probs, top_indices):
        token = tokenizer.decode([idx.item()])
        print(f"Token: {repr(token)} - Prob: {prob.item():.4f}")

Output:
```
=== PROMPT ===
Who is Obama?

=== TOKENS ===
['Who', 'ƒ†is', 'ƒ†Obama', '?']

=== GENERATION DEBUG ===

--- Step 1 ---
Token: '\n'            | Prob: 0.3131
Token: ' What'         | Prob: 0.0408
Token: ' Why'          | Prob: 0.0254
Token: ' Who'          | Prob: 0.0240
Token: ' The'          | Prob: 0.0238
Current output: Who is Obama?


--- Step 2 ---
Token: '\n'            | Prob: 0.9846
Token: 'The'           | Prob: 0.0021
Token: 'In'            | Prob: 0.0007
Token: 'I'             | Prob: 0.0007
Token: 'This'          | Prob: 0.0006
Current output: Who is Obama?


--- Step 3 ---
Token: 'Obama'         | Prob: 0.1506
Token: 'The'           | Prob: 0.0894
Token: 'In'            | Prob: 0.0314
Token: 'It'            | Prob: 0.0222
Token: '"'             | Prob: 0.0221
Current output: Who is Obama?

Obama

--- Step 4 ---
Token: ' is'           | Prob: 0.2831
Token: ' has'          | Prob: 0.1094
Token: "'s"            | Prob: 0.0875
Token: ' was'          | Prob: 0.0668
Token: ','             | Prob: 0.0537
Current output: Who is Obama?

Obama is

--- Step 5 ---
Token: ' the'          | Prob: 0.2499
Token: ' a'            | Prob: 0.2078
Token: ' not'          | Prob: 0.0794
Token: ' an'           | Prob: 0.0417
Token: ' Obama'        | Prob: 0.0192
Current output: Who is Obama?

Obama is the

--- Step 6 ---
Token: ' president'    | Prob: 0.1474
Token: ' most'         | Prob: 0.0543
Token: ' first'        | Prob: 0.0492
Token: ' only'         | Prob: 0.0389
Token: ' President'    | Prob: 0.0301
Current output: Who is Obama?

Obama is the president

--- Step 7 ---
Token: ' of'           | Prob: 0.5815
Token: ' who'          | Prob: 0.1236
Token: '.'             | Prob: 0.0478
Token: ','             | Prob: 0.0376
Token: '-'             | Prob: 0.0302
Current output: Who is Obama?

Obama is the president of

--- Step 8 ---
Token: ' the'          | Prob: 0.8773
Token: ' a'            | Prob: 0.0176
Token: ' America'      | Prob: 0.0105
Token: ' our'          | Prob: 0.0062
Token: ' United'       | Prob: 0.0058
Current output: Who is Obama?

Obama is the president of the

--- Step 9 ---
Token: ' United'       | Prob: 0.9511
Token: ' U'            | Prob: 0.0059
Token: ' world'        | Prob: 0.0059
Token: ' USA'          | Prob: 0.0041
Token: ' US'           | Prob: 0.0036
Current output: Who is Obama?

Obama is the president of the United

--- Step 10 ---
Token: ' States'       | Prob: 0.9983
Token: ' State'        | Prob: 0.0004
Token: ' Nations'      | Prob: 0.0004
Token: ' Kingdom'      | Prob: 0.0002
Token: ' Arab'         | Prob: 0.0001
Current output: Who is Obama?

Obama is the president of the United States

=== FINAL OUTPUT ===
Who is Obama?

Obama is the president of the United States
```

* * *

24\. T·ªïng k·∫øt (TL;DR)
----------------------

*   Prompt l√† string
*   Prompt engineering gi·ªëng vi·∫øt code
*   LLM d·ª± ƒëo√°n token ti·∫øp theo d·ª±a tr√™n x√°c su·∫•t
*   Token l√† ƒë∆°n v·ªã x·ª≠ l√Ω c∆° b·∫£n, kh√¥ng ph·∫£i t·ª´ hay k√Ω t·ª±
*   Context window l√† "RAM" c·ªßa LLM - c√≥ gi·ªõi h·∫°n token
*   Token b·ªã c·∫Øt ‚Üí m·∫•t context ‚Üí model hallucinate
*   Mu·ªën d·ªØ li·ªáu m·ªõi nh·∫•t: d√πng RAG
